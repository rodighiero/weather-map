{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contours\n",
    "\n",
    "from random import random\n",
    "\n",
    "\n",
    "def f(x, y):\n",
    "    return np.sin(x) ** 10 + np.cos(10 + y * x) * np.cos(x)\n",
    "\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "Z = np.zeros((len(X),len(Y)))\n",
    "\n",
    "\n",
    "for indexX, elX in enumerate(X):\n",
    "    for indexY, elY in enumerate(Y):\n",
    "        # continue\n",
    "        # print(random() * 1000)\n",
    "        Z[indexX][indexY] = random() * 100\n",
    "Z\n",
    "\n",
    "# The proble is that I want to use \"slopes\" values which are monodimensional and not bidimensional\n",
    "\n",
    "plt.contour(X, Y, Z, colors='black', linewidths=.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "iris = sns.load_dataset(\"iris\")\n",
    "\n",
    "# Set up the figure\n",
    "f, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.set_aspect(\"equal\")\n",
    "\n",
    "# Draw a contour plot to represent each bivariate density\n",
    "sns.kdeplot(\n",
    "    # data=iris.query(\"species != 'versicolor'\"),\n",
    "    x=x,\n",
    "    y=y,\n",
    "    # hue=\"species\",\n",
    "    thresh=.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "            # tag = tag.replace(', Inc', '')\n",
    "            # tag = tag.replace(' Co', '')\n",
    "            # tag = tag.replace('US ', '')\n",
    "            # tag = tag.replace('mmission', ' Commission')\n",
    "            # tag = tag.replace('llege', ' College')\n",
    "            # tag = tag.replace('mmerce', ' Commerce')\n",
    "            # tag = tag.replace('lumbia', ' Columbia')\n",
    "            # tag = tag.replace('ourt', ' Court')\n",
    "            # tag = tag.replace('rporation', ' Corporation')\n",
    "            # tag = tag.replace('eurt', ' Coeurt')\n",
    "            \n",
    "            # if tag == 'ExxonMobil': tag = 'Exxon'\n",
    "            # if tag == 'Exxon Mobil': tag = 'Exxon'\n",
    "            # if tag == 'Interior': tag = 'Interior Department'\n",
    "            # if tag == 'Reuters': tag = 'Thomson Reuters'\n",
    "            # if tag == 'Royal Dutch Shell': tag = 'Shell'\n",
    "            # if tag == 'World Health Organisation': tag = 'World Health Organization'\n",
    "            # if tag == 'International Monetary Found': tag = 'International Monetary Fund'\n",
    "            # if tag == 'European commission': tag = 'European Commission'\n",
    "            # if tag == 'Department of Agriculture': tag = 'Agriculture Department'\n",
    "            # if tag == 'Department of Energy': tag = 'Energy Department'\n",
    "            # if tag == 'State': tag = 'State Department'\n",
    "            # if tag == 'House': tag = 'White House'\n",
    "            # if tag == 'Sierra': tag = 'Sierra Club'\n",
    "            \n",
    "            # if tag == 'Yale': tag = 'Yale University'\n",
    "            # if tag == 'Penn State': tag = 'Pennsylvania State University'\n",
    "            # if tag == 'Oxford of University': tag = 'University Oxford'\n",
    "            # if tag == 'Oxford': tag = 'Oxford University'\n",
    "            # if tag == 'Stanford': tag = 'Stanford University'\n",
    "            # if tag == 'Massachusetts Institute of Technology': tag = 'MIT'\n",
    "            # if tag == 'Harvard': tag = 'Harvard University'\n",
    "\n",
    "  \n",
    "            # \n",
    "            # \n",
    "\n",
    "            # if ('University' or 'College' or 'Institute') in tag:\n",
    "            #     continue\n",
    "\n",
    "            # stoplist = {'Inc', 'Amazon rainforest', 'Agriculture', 'Energy', 'Ltd', 'Republican', 'MIT', 'Times', 'Parliament', 'UN', 'Liverpool'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## New total linear regression\n",
    "\n",
    "y = list(recordsByYear.values())\n",
    "x = list(recordsByYear.keys())\n",
    "x = np.array(x).reshape((-1, 1))\n",
    "\n",
    "model = LinearRegression().fit(x, y)\n",
    "slope = model.coef_\n",
    "total_slope = slope[0]\n",
    "\n",
    "'Total linear regression is {}'.format(total_slope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # for i, tag in enumerate(tags):\n",
    "\n",
    "    #     if tag == 'Obama': tags[i] = 'Barack Obama'\n",
    "    #     if tag == 'Trump': tags[i] = 'Donald Trump'\n",
    "    #     if tag == 'USA TODAY': tags[i] = 'USA Today'\n",
    "\n",
    "    #     if tag == 'NOAA': tags[i] = 'National Oceanic and Atmospheric Administration'\n",
    "    #     if tag == 'EPA': tags[i] = 'Environmental Protection Agency'\n",
    "    #     if tag == 'United States Environmental Protection Agency': tags[i] = 'Environmental Protection Agency'\n",
    "    #     if tag == 'UN': tags[i] = 'United Nations'\n",
    "    #     if tag == 'EC': tags[i] = 'European Commission'\n",
    "    #     if tag == 'IEA': tags[i] = 'International Energy Agency'\n",
    "    #     if tag == 'EU': tags[i] = 'European Union'\n",
    "    #     if tag == 'EIA': tags[i] = 'Energy Information Administration'\n",
    "    #     if tag == 'AP': tags[i] = 'Associated Press'\n",
    "    #     if tag == 'ASSOCIATED PRESS': tags[i] = 'Associated Press'\n",
    "    #     if tag == 'DOE': tags[i] = 'Department of Energy'\n",
    "    #     if tag == 'Energy': tags[i] = 'Department of Energy'\n",
    "    #     if tag == 'Energy Department': tags[i] = 'Department of Energy'\n",
    "    #     if tag == 'GM': tags[i] = 'General Motors'\n",
    "    #     if tag == 'IRS': tags[i] = 'Internal Revenue Service'\n",
    "    #     if tag == 'GOP': tags[i] = 'Republican Party'\n",
    "    #     if tag == 'USDA': tags[i] = 'Department of Agriculture'\n",
    "    #     if tag == 'Agriculture Department': tags[i] = 'Department of Agriculture'\n",
    "    #     if tag == 'TPWD': tags[i] = 'Texas Parks and Wildlife Department'\n",
    "    #     if tag == 'DNR': tags[i] = 'Department of Natural Resources'\n",
    "    #     if tag == 'FDA': tags[i] = 'Food and Drug Administration'\n",
    "    #     if tag == 'OPEC': tags[i] = 'Organization of the Petroleum Exporting Countries'\n",
    "    #     if tag == 'VW': tags[i] = 'Volkswagen'\n",
    "    #     if tag == 'EDF': tags[i] = 'EDF Energy'\n",
    "    #     if tag == 'RHI': tags[i] = 'Renewable Heat Incentive'\n",
    "        \n",
    "    #     if tag == 'Xcel': tags[i] = 'Xcel Energy'\n",
    "    #     if tag == 'Stanford': tags[i] = 'Stanford University'\n",
    "    #     if tag == 'Harvard': tags[i] = 'Harvard University'\n",
    "    #     if tag == 'Yale': tags[i] = 'Yale University'\n",
    "    #     if tag == 'Nasa': tags[i] = 'NASA'\n",
    "    #     if tag == 'MSP': tags[i] = 'Minneapolis–Saint Paul'\n",
    "    #     if tag == 'MinneapolisSaint Paul': tags[i] = 'Minneapolis–Saint Paul'\n",
    "    #     if tag == 'St Paul': tags[i] = 'Minneapolis–Saint Paul'\n",
    "\n",
    "    #     if tag.startswith('University of California'): tags[i] = 'University of California'\n",
    "    #     if tag.startswith('Columbia University'): tags[i] = 'Columbia University'\n",
    "    #     if tag.startswith('Bloomberg'): tags[i] = 'Bloomberg'\n",
    "    #     if tag.startswith('UBS'): tags[i] = 'UBS'\n",
    "    #     if tag.startswith('Exxon'): tags[i] = 'Exxon'\n",
    "    #     if tag.startswith('Drax'): tags[i] = 'Drax'\n",
    "    #     if tag.startswith('Valens'): tags[i] = 'Valens'\n",
    "    #     if tag.startswith('Siemens'): tags[i] = 'Siemens'\n",
    "    #     if tag.startswith('Abengoa'): tags[i] = 'Abengoa'\n",
    "    #     if tag.startswith('BBC'): tags[i] = 'BBC'\n",
    "    #     if tag.startswith('New York Times'): tags[i] = 'New York Times'\n",
    "\n",
    "    #     if tag == 'Sierra': tags[i] = 'Sierra Club'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "\n",
    "occurences = []\n",
    "orgs = []\n",
    "texts = []\n",
    "type = []\n",
    "urls = []\n",
    "years = []\n",
    "\n",
    "years_template = { 2011: 0, 2012: 0, 2013: 0, 2014: 0, 2015: 0, 2016: 0, 2017: 0, 2018: 0, 2019: 0, 2020: 0 }\n",
    "\n",
    "recordsByYear = years_template.copy()\n",
    "\n",
    "dir = 'data/biomass/'; files = os.listdir(dir) # Set folder and files\n",
    "\n",
    "for index, filename in enumerate(files):\n",
    "\n",
    "    if not index % 1000: print(index, ' | ', end=' ') # Counter\n",
    "    \n",
    "    f = open(dir + filename); r = f.read(); data = json.loads(r) # Read file\n",
    "\n",
    "    try:\n",
    "\n",
    "        url = data[0]['url'] # Set URL\n",
    "        records = data[0]['story_tags'] # Set tags\n",
    "        length = len(records) # Set length\n",
    "        year = int(data[0]['publish_date'].split(' ')[0].split('-')[0]) # Set year\n",
    "        \n",
    "        if any(x in url for x in {'feeds.', 'rss.', 'briefing'}): continue # Blacklist (Bob Dylan's case)\n",
    "        \n",
    "        if not any(x in url for x in {'wsj.com', 'usatoday.com', 'nytimes.com', 'latimes.com', 'nypost.com', 'washingtonpost.com', 'chicagotribune.com', 'chron.com', 'nydailynews.com', 'theguardian.com'}): continue # Whitelist\n",
    "\n",
    "        if  length > 100: continue # Set max\n",
    "\n",
    "    except:\n",
    "\n",
    "        # In case of error\n",
    "\n",
    "        continue\n",
    "\n",
    "\n",
    "    # compute the total\n",
    "\n",
    "    recordsByYear[year] += 1\n",
    "\n",
    "\n",
    "\n",
    "    # Collect entities\n",
    "    \n",
    "    tags = []\n",
    "\n",
    "    for record in records:\n",
    "        \n",
    "        if any(x in record['tag_set'] for x in {'nyt_labels', 'cliff_organizations', 'cliff_people'}):\n",
    "            \n",
    "            tag = record['tag']\n",
    "\n",
    "            tag = ' '.join(tag.split()) # Merge multiple spaces\n",
    "            tag = tag.replace('.', '') # Remove dots\n",
    "            tag = tag.replace(',', '') # Remove commas\n",
    "            \n",
    "            if tag.startswith('US '): tag = tag[3:] # Remove prefix\n",
    "            if tag.endswith(' Corp'): tag = tag[:-5] # Remive postfix\n",
    "            if tag.endswith(' Inc'): tag = tag[:-4] # Remive postfix\n",
    "            \n",
    "            if tag[0] == ('—'): continue # Remove listings\n",
    "\n",
    "            if tag[1].isupper(): continue # Remove acronyms\n",
    "\n",
    "            tags.append(tag)\n",
    "    \n",
    "    \n",
    "    tags = list(filter(lambda x: not x in {'nyt_labeller_v100', 'no index terms from nytimes', 'post', 'Post', 'Google', 'Facebook', 'Amazon', 'YouTube', 'brown', 'Reuters', 'New York Times', 'BBC'}, tags)) # Blacklist  \n",
    "\n",
    "    tags = list(set(tags)) # Remove duplicates\n",
    "\n",
    "\n",
    "    # Create data structure\n",
    "\n",
    "    for t in tags:\n",
    "\n",
    "        related = tags.copy(); related.remove(t); # Copy related tags and remove the selected one\n",
    "        \n",
    "        if t not in orgs: # Add element\n",
    "            orgs.append(t)\n",
    "            i = orgs.index(t)\n",
    "            texts.append(related)\n",
    "            urls.append([url])\n",
    "            occurences.append(1)\n",
    "            years.append(years_template.copy())\n",
    "            years[i][year] += 1\n",
    "\n",
    "        # Update element\n",
    "                \n",
    "        else:\n",
    "            i = orgs.index(t)\n",
    "            texts[i] += related\n",
    "            urls[i].append(url)\n",
    "            occurences[i] += 1\n",
    "            years[i][year] += 1\n",
    "\n",
    "    # if index > 20:\n",
    "    #     raise SystemExit(\"Stop right there!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     if i == 18:\n",
    "#         print(year, slope)\n",
    "#         plt.xlabel(\"Slope is\" + str(slope))\n",
    "#         plt.scatter(x, y)\n",
    "\n",
    "\n",
    "\n",
    "# max = np.max(slopes)\n",
    "# min = np.min(slopes)\n",
    "\n",
    "# means = np.mean(slopes) / 2 # Means is divided by two to balance positive and negatives\n",
    "# means = np.mean(slopes) # Standard means\n",
    "\n",
    "# for index, slope in enumerate(slopes):\n",
    "#     slopes[index] = slope - means\n",
    "\n",
    "# slopes[100], orgs[100], years[100]\n",
    "\n",
    "# max, min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hulls\n",
    "\n",
    "# for cluster in clusters:\n",
    "\n",
    "    # Average color\n",
    "\n",
    "    # background_color = []\n",
    "\n",
    "    # for i, index in enumerate(cluster):\n",
    "    #     for occurence in range(occurences[index]):\n",
    "    #         background_color.append(\n",
    "    #             [colors[index][0], colors[index][1], colors[index][2]]\n",
    "    #         )\n",
    "\n",
    "    # r = [i[0] for i in background_color]\n",
    "    # r = sum(r) / len(r)\n",
    "    # g = [i[1] for i in background_color]\n",
    "    # g = sum(g) / len(g)\n",
    "    # b = [i[2] for i in background_color]\n",
    "    # b = sum(b) / len(b)\n",
    "\n",
    "    # background_color = (r, g, b, 1)\n",
    "\n",
    "    # Hull\n",
    "\n",
    "    # points = []\n",
    "    # for index in cluster:\n",
    "    #     points.append([embedding[index][0], embedding[index][1]])\n",
    "    # points = np.array(points)\n",
    "\n",
    "    # # print(points)\n",
    "\n",
    "    # hull = ConvexHull(points)\n",
    "\n",
    "    # x_hull = np.append(\n",
    "    #     points[hull.vertices, 0], points[hull.vertices, 0][0]\n",
    "    # )  # Collect the xs + first x\n",
    "    # y_hull = np.append(points[hull.vertices, 1], points[hull.vertices, 1][0])\n",
    "\n",
    "    # # print(x_hull)\n",
    "\n",
    "    # # break\n",
    "\n",
    "    # # Interpolation\n",
    "\n",
    "    # dist = np.sqrt((x_hull[:-1] - x_hull[1:]) ** 2 + (y_hull[:-1] - y_hull[1:]) ** 2)\n",
    "    # dist_along = np.concatenate(([0], dist.cumsum()))\n",
    "    # spline, u = interpolate.splprep([x_hull, y_hull], u=dist_along, s=0)\n",
    "    # interp_d = np.linspace(dist_along[0], dist_along[-1], 50)\n",
    "    # interp_x, interp_y = interpolate.splev(interp_d, spline)\n",
    "\n",
    "    # # plot shape\n",
    "    # plt.fill(interp_x, interp_y, \"--\", c=background_color, alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSONs\n",
    "\n",
    "with open(\"src/data/years.json\", \"w\", encoding=\"UTF8\") as file:\n",
    "    json.dump(entities_filtered['years'].to_list(), file)\n",
    "\n",
    "with open(\"src/data/urls.json\", \"w\", encoding=\"UTF8\") as file:\n",
    "    json.dump(entities_filtered['urls'].to_list(), file)\n",
    "\n",
    "\n",
    "# CSVs\n",
    "\n",
    "with open(\"src/data/clusters.csv\", \"w\", encoding=\"UTF8\") as file:\n",
    "    file.write(\"cluster\\n\")\n",
    "    for indexEmbedding, element in enumerate(embedding):\n",
    "        value = None\n",
    "        for indexCluster, cluster in enumerate(clusters):\n",
    "            if indexEmbedding in cluster:\n",
    "                value = indexCluster\n",
    "        file.write(str(value) + \"\\n\")\n",
    "\n",
    "with open(\"src/data/embedding.csv\", \"w\", encoding=\"UTF8\") as f:\n",
    "    f.write(\"x,y\\n\")\n",
    "    for element in embedding:\n",
    "        f.write(str(element[0]) + \",\" + \"-\" + str(element[1]) + \"\\n\")\n",
    "\n",
    "with open(\"src/data/frequency.csv\", \"w\", encoding=\"UTF8\") as f:\n",
    "    f.write(\"frequency\\n\")\n",
    "    for element in entities_filtered['frequency'].to_list():\n",
    "        f.write(str(element) + \"\\n\")\n",
    "\n",
    "with open(\"src/data/names.csv\", \"w\", encoding=\"UTF8\") as f:\n",
    "    f.write(\"name\\n\")\n",
    "    for element in entities_filtered['name'].to_list():\n",
    "        f.write(str(element) + \"\\n\")\n",
    "\n",
    "with open(\"src/data/regressions.csv\", \"w\", encoding=\"UTF8\") as f:\n",
    "    f.write(\"regression\\n\")\n",
    "    for element in entities_filtered['slope'].to_list():\n",
    "        f.write(str(element) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping by cluster\n",
    "\n",
    "values = set(clusters)\n",
    "\n",
    "if -1 in values:\n",
    "    values.remove(-1)\n",
    "\n",
    "clusters = [\n",
    "    [index for index, cluster in enumerate(clusters) if cluster == value]\n",
    "    for value in values\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # for i, index in enumerate(cluster):\n",
    "    #     for occurence in range(occurences[index]):\n",
    "    #         background_color.append(\n",
    "    #             [colors[index][0], colors[index][1], colors[index][2]]\n",
    "    #         )\n",
    "\n",
    "    # r = [i[0] for i in background_color]\n",
    "    # r = sum(r) / len(r)\n",
    "    # g = [i[1] for i in background_color]\n",
    "    # g = sum(g) / len(g)\n",
    "    # b = [i[2] for i in background_color]\n",
    "    # b = sum(b) / len(b)\n",
    "\n",
    "    # background_color = (1, 1, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # Interpolation\n",
    "\n",
    "    # dist = np.sqrt((x_hull[:-1] - x_hull[1:]) ** 2 + (y_hull[:-1] - y_hull[1:]) ** 2)\n",
    "    # dist_along = np.concatenate(([0], dist.cumsum()))\n",
    "    # spline, u = interpolate.splprep([x_hull, y_hull], u=dist_along, s=0)\n",
    "    # interp_d = np.linspace(dist_along[0], dist_along[-1], 50)\n",
    "    # interp_x, interp_y = interpolate.splev(interp_d, spline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # if not any(\n",
    "        #     x in url\n",
    "        #     for x in {\n",
    "        #         \"wsj.com\",\n",
    "        #         \"usatoday.com\",\n",
    "        #         \"nytimes.com\",\n",
    "        #         \"latimes.com\",\n",
    "        #         \"nypost.com\",\n",
    "        #         \"washingtonpost.com\",\n",
    "        #         \"chicagotribune.com\",\n",
    "        #         \"chron.com\",\n",
    "        #         \"nydailynews.com\",\n",
    "        #         \"theguardian.com\",\n",
    "        #     }\n",
    "        # ):\n",
    "        #     continue  # Whitelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old Clustering\n",
    "\n",
    "# clusterer = hdbscan.HDBSCAN(\n",
    "#     min_samples=4,  # min_samples = number of close elements to create a set\n",
    "#     min_cluster_size=3,\n",
    "#     gen_min_span_tree=False,\n",
    "#     leaf_size=3,\n",
    "#     cluster_selection_epsilon=0.3,  # cluster_selection_epsilon = radius\n",
    "# )\n",
    "\n",
    "# entities_nosubjects = entities.loc[entities[\"type\"] != \"subject\"]\n",
    "# embedding_nosubjects = entities_nosubjects[[\"x\", \"y\"]].to_numpy()\n",
    "\n",
    "# clusterer.fit(embedding_nosubjects)\n",
    "# clusters = clusterer.labels_\n",
    "# # clusters = np.where(clusters == -1, None, clusters)  # Replace -1 with None\n",
    "\n",
    "# entities = entities.assign(cluster=-1)  # Clean the cluster columns\n",
    "\n",
    "# for count, (index, row) in enumerate(entities_nosubjects.iterrows()):\n",
    "#     entities.loc[index, \"cluster\"] = clusters[count] # Write cluster number\n",
    "\n",
    "# print(entities.sample(1)[[\"name\", \"cluster\"]].to_numpy()) # Print one sample\n",
    "\n",
    "\n",
    "# # Plotting\n",
    "\n",
    "# plt.figure(figsize=(5, 5))\n",
    "# plt.axis(\"off\")\n",
    "\n",
    "# clusters = set(clusters.tolist()) # Make the list of clusters\n",
    "# clusters.remove(-1) # Remove None\n",
    "\n",
    "# for counter, cluster in enumerate(clusters):\n",
    "\n",
    "#     rows = entities.loc[entities[\"cluster\"] == cluster] # Rows\n",
    "\n",
    "#     points = []  # Points\n",
    "#     for index, row in rows.iterrows():\n",
    "#         points.append([row[\"x\"], row[\"y\"]])\n",
    "#     points = np.array(points)\n",
    "\n",
    "#     hull = ConvexHull(points)  # Hull\n",
    "#     x_hull = np.append(points[hull.vertices, 0], points[hull.vertices, 0][0])\n",
    "#     y_hull = np.append(points[hull.vertices, 1], points[hull.vertices, 1][0])\n",
    "\n",
    "#     plt.fill(x_hull, y_hull, \"--\", c=\"#b40426\", alpha=0.5) # Plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "clusters = set(entities[\"cluster\"].tolist())\n",
    "clusters.remove(\"-1\")  # Remove -1\n",
    "\n",
    "# print(clusters)\n",
    "\n",
    "for counter, cluster in enumerate(clusters):\n",
    "\n",
    "    if counter == 0:\n",
    "\n",
    "        x = np.empty((0, 1), dtype=int)\n",
    "        y = np.empty((0, 1), dtype=int)\n",
    "\n",
    "        rows = entities.loc[entities[\"cluster\"] == cluster]  # Rows\n",
    "\n",
    "        for index, row in rows.iterrows():\n",
    "            x = np.append(x, row[\"x\"])\n",
    "            y = np.append(y, row[\"y\"])\n",
    "\n",
    "        print('x', x)\n",
    "        print('y', y)\n",
    "        # print()\n",
    "\n",
    "        # fit a curve to the data using a least squares 1st order polynomial fit\n",
    "        z = np.polyfit(x, y, 1)\n",
    "        p = np.poly1d(z)\n",
    "        fit = p(x)\n",
    "\n",
    "        print('z', z)\n",
    "        print('p', p)\n",
    "        print('fit', fit)\n",
    "\n",
    "        # get the coordinates for the fit curve\n",
    "        c_y = [np.min(fit), np.max(fit)]\n",
    "        c_x = [np.min(x), np.max(x)]\n",
    "\n",
    "        # predict y values of origional data using the fit\n",
    "        p_y = z[0] * x + z[1]\n",
    "\n",
    "        # calculate the y-error (residuals)\n",
    "        y_err = y - p_y\n",
    "\n",
    "        # create series of new test x-values to predict for\n",
    "        p_x = np.arange(np.min(x), np.max(x) + 1, 1)\n",
    "\n",
    "        # now calculate confidence intervals for new test x-series\n",
    "        mean_x = np.mean(x)  # mean of x\n",
    "        n = len(x)  # number of samples in origional fit\n",
    "        t = 2.31  # appropriate t value (where n=9, two tailed 95%)\n",
    "        s_err = np.sum(np.power(y_err, 2))  # sum of the squares of the residuals\n",
    "\n",
    "        confs = t * np.sqrt(\n",
    "            (s_err / (n - 2))\n",
    "            * (\n",
    "                1.0 / n\n",
    "                + (\n",
    "                    np.power((p_x - mean_x), 2)\n",
    "                    / ((np.sum(np.power(x, 2))) - n * (np.power(mean_x, 2)))\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # now predict y based on test x-values\n",
    "        p_y = z[0] * p_x + z[0]\n",
    "\n",
    "        # get lower and upper confidence limits based on predicted y and confidence intervals\n",
    "        lower = p_y - abs(confs)\n",
    "        upper = p_y + abs(confs)\n",
    "\n",
    "        # plot sample data\n",
    "        plt.plot(x, y, \"bo\", label=\"Sample observations\")\n",
    "\n",
    "        # plot line of best fit\n",
    "        plt.plot(c_x, c_y, \"r-\", label=\"Regression line\")\n",
    "\n",
    "        # # plot confidence limits\n",
    "        plt.plot(p_x, lower, \"b--\", label=\"Lower confidence limit (95%)\")\n",
    "        plt.plot(p_x, upper, \"b--\", label=\"Upper confidence limit (95%)\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
